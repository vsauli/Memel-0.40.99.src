{"msgnr": 1, "lang": "EN", "msg": " The development of Project Memel has started. There were 3 versions/attempts some time before, which were unsuccessful because I had no time to dedicate for the development. I tried various technologies for JS server and underlying database and, finally, I came up with using NodeJS and MongoDB for the current project version. The original idea for this project is based on my <a href='http://www.prodata.lt/EN/Programming/OPU_computing_model.pdf'>paper</a> about the possibility to parallelize Javascript code using a bunch of interconnected servers and workers (Object Processing Units - OPUs).<br> <br>The frontend for the Project is implemented as a Web SPA (Single Page Application) and uses ExtJS V3 widget library for GUI interface. I adopted an ACE web-based text editor for viewing and editing parallel JS executables and other files.<br> <br>The backend - the Parallel Javascript Machine - is using NodeJS, and implemented as a Web server for the frontend, as well as a server for the OPUs that really do the parallel execution of code. OPUs are small Javascript network clients implemented in NodeJS too. There may be pretty much of them, connected to the main parallel machine server either locally or remotely. The overall performance of parallel processing strongly depends on the number of connected OPUs.<br> <br> All these parts, working together, may be treated as a mini-OS which launches and parses the running scripts (tasks), puts them into system execution queue and provides some kind of cooperative multitasking. The results are printed to the Web client's console by pipelining <code>console.log</code> output from OPUs through the main server. <br> <br> Currently, at the version 4.0.0, the works are concentrated on a Web client software, which is responsible for creating, editing and launching parallel Javascript tasks. <br> <br> --- Vladas Saulis<br> --- System architect and creator.<br>", "subj": "Project Memel 4.0 development has started!", "to_user": "@all", type: "P", sts: "", cdate: "17/01/30", ctime: 78565, fr_user: "vladas@system", cat: "M"}
{"msgnr": 2, "lang": "EN", "msg": " The Web GUI client is almost finished. Now system kernel development has started. Like any other OS, our mini-OS must have a kernel. The kernel operates with such internal structures as the Queue object, Task list and Task variables object.<br> <br> When the task is launched, its source code passed to <code>Parallelizator</code> function, where it is separated into single statements (also can be grouped in blocks). Every statement or block then is enqueued into the system's Queue for execution by OPUs, which are connected to the server and extract these statements from the Queue one by one in no predetermined order. So we may say that the tasks are parallelized with the <i>statement granularity</i>. The parallelization process is controlled by special instructions (or hints), called <code>#pragmas</code>. Some of them sets the start of parallel/sequential code, other sets the beginnig/end of block, parallel variables (so called <i>Parvars</i>), begin/end of the process timing. The one of them, <code>#pragma wait</code>, is very important for the parallellel execution flow control, and marks the places of the <i>assembly points</i> in the task. At this point the parallelization of the task is suspended until all pending parallel chunks of code finish their execution (for parallel loops it waits for all iterations to be complete). Then the process of parallelization resumes from this point. The 'waiting' process in our system is really non-blocking, letting other parts of the task to continue their execution, what is exactly opposite to normal one-threaded Javascript process. <br> <br> During the 'parallelization', when some statements contain an internal block of statements (such as loops, functions, if-then-else blocks), the <code>Analizator</code> function is called. It provides more specialized interpretaion of code logic. For example, it interprets '<code>for</code>' loop's initial expression, condition and step. This information then is added to the parallel code chunks as an extra internal code. <br> <br> Another important part of parallel tasks execution is the task variables handling. All task variables are stored to the server's Task object. Before the task chunk is placed into the Queue, the <code>Parallelizator</code> function determines which variables are in use within this chunk. It puts these variables along with their values as a definition to the preamble of the code. After execution of the code chunk, the resulting values of variables are returned and set back to the server's task object. There are some special <i>parallel variables</i> (<code>Parvars</code>), which are handled differently. These variables are <i>declared</i> by <code>#pragma parvar [var name]</code> special instruction. This means, that these variables can be accessed and set concurrently. All 4 basic arithmetic operations are transitive (if processed one by one), so they can be processed in any order, and the result will always be the same. The parallel 'for' loop works exactly in this way. So the accumulator variable for the loop must be set in the way that it could be accessed and modified concurrently. This is what <code>Parvars</code> are for. The kernel adds additional code fragments that sinchronously calls server for variable value and locks it in order to be modified after the code chunk finishes its execution. It is important that the code chunk would be fast and atomic, so it would lock the parvar for as short time as possible. <br> <br> This is in a few words what I plan to implement in the next months. Hope it won't take a long time.", "subj": "Kernel development", "to_user": "@all", type: "P", sts:  "", cdate: "17/04/24", ctime: 79332, fr_user: "vladas@system", cat: "M"}
{"msgnr": 3, "lang": "EN", "msg": " OPU (Object Processing Unit) is a lightweight networking client (worker) which does the actual processing of parallelized chunks of code. OPU connects to main server and scans the Queue for the next job in FIFO manner. When it founds the job, it executes it, returns the result (task variables) and scans again. If no jobs are found it enters an idle loop and waits. OPUs are pro-active, because they initiate job extraction and processing. There may be an unlimited number of OPUs, connected to the main server, working independently. Their number is only limited by server's processing power and the network speed.<br> <br> OPUs have another very important function. When they meet <code>Parvar</code> instruction in the code, they connect directly to the main server to get realtime value of Parvar, and when Parvar has changed they connect again and set its changed value to the corresponding task variable. While OPUs process Parvars in such way, these are locked on the server to prevent race condition. Thus, it could be said, that the system has data bound control flow, which is opposite to the statement level control flow as it's usual for all non-parallel systems.<br> <br> When the OPU development will be finished, it will become possible to make the first <i>Proof of Concept</i> tests.", "subj": "OPU implementation", "to_user": "@all", type: "P", sts: "", cdate: "17/06/10", ctime: 1000, fr_user: "vladas@system", cat: ""}
{"msgnr": 4, "lang": "EN", "msg": " Multi-user access is very important for this system. It lets users to launch tasks independently and asynchronously. Every task in the system belongs to a particular user, and only that user can see his task's output. When some user is registered in the system, he gets his own <i>workspace</i> with pre-installed examples. He can then modify and save any file in his workspace, as well as to create new. The user's launched tasks execute in parallel. It depends on how often tasks waits for parallelization in <i>assembly points</i> (look at the previous news for this term). Such task behaviour is often called <i>cooperative multitasking</i>. First versions of Apple OS worked in similar way. All tasks in the system must give a 'breath' for all other tasks in the system, and the <code>#pragma wait</code> instruction is a way to do so.", "subj": "Multiuser access", "to_user": "@all", type: "P", sts: "", cdate: "17/06/10", ctime: 1000, fr_user: "vladas@system", cat: ""}
{"msgnr": 5, "lang": "EN", "msg": " Good news! The first <i>Proof of Concept</i> tests have passed successfully! The example of parallel <code>for</code> loop was tested with 1 and 2 OPUs on 2 processors Linux machine. It took 23.6 seconds with 1 OPU, and 13.5 seconds with 2 OPUs, each on separate CPU. It shows nearly linear performace gain for parallel <code>for</code> loop. Now it's planned to expose the whole Project Memel to the 'wild' for the public tests. 8 OUPs will be set up on 8 CPU machine, and the main server on the other. I'm planning to finish all works to the end of August, 2017. As from now I have changed the version numbering. New versions will show the percentage of works done. Currently, the vesion is set to 0.10.14 alpha. The last number shows the build number. The Project is still in early alpha stage and pretends only for a 'Technology Preview'. If the community finds it useful in real parallel 'number-crunching' applications, the development will be continued.", "subj": "The first tests", "to_user": "@all", type: "P", sts: "", cdate: "17/06/12", ctime: 1000, fr_user: "vladas@system", cat: ""}
{"msgnr": 6, "lang": "EN", "msg": " This is the first public release of Project Memel - Parallel Javascript Machine. The Web GUI and the main server reside on this server. 8 OPUs are launched on the other machine (in local network) and utilize 8 CPUs. By now, there are 3 examples available for testing. Each example exists in two variants - parallel and sequential, for better execution time comparations. Happy testing, folks!", "subj": "Project Memel 0.15.40 is out to the public", "to_user": "@all", type: "P", sts: "", cdate: "17/09/04", ctime: 1000, fr_user: "vladas@system", cat: ""}
{"msgnr": 7, "lang": "EN", "msg": " Version 0.15.41. Trivial examples are placed under the <code>trivia</code> directory. There are 5 pairs of examples - one of sequential and one of parallel mode for each pair. The examples show how to use various <code>#pragma</code> directives in some standard situations. All examples print execution times to console to clearly show the parallel mode advantage against the sequential mode counterparts. We can therefore confirm that the parallel mode execution scales almost linearly with the increased nubmer of CPUs (OPUs) being attached.", "subj": "Trivial examples are ready", "to_user": "@all", type: "P", sts: "", cdate: "18/05/13", ctime: 1000, fr_user: "vladas@system", cat: ""}
{"msgnr": 8, "lang": "EN", "msg": " Version 0.23.51. Linear Programming(LP) or linear optimisation is not a very good subject for parallel processing, because all known algorithms for LP solution are iterative. But, nevertheless, parallel processing may take place even with iterative calculations. Some iterative algorithms have a certain number of parameters for each iteration, which can be variated to some extent. Let's call it a psi-factor. The idea is to launch several variants of a particular iteration in parallel, applying a slightly different psi-factor to each iteration variant. In such case we can then choose the result, which gives us a minimum (or maximum) target function's value of all iteration's variants, and start the next iteration depending on this result. As first test showed, this approach dramatically improves the convergention of the whole algorithm, reducing the overall number of iterations by the order of magnitude. Although the time of performing several iteration variants in parallel is equal to the longest variant running time plus parallel processing overhead, the overall process of LP solution takes less time, because of better convergention and lesser number of such 'combined' iterations. During the latest tests I reached around 1.5 times gain in parallel LP algorithm performance for matrices from 100x500 to 300x500 with randomly generated elements. I'll continue the tests and expose the results to my examples space as soon as possible.", "subj": "Linear programming examples expected soon", "to_user": "@all", type: "P", sts: "", cdate: "19/01/08", ctime: 1000, fr_user: "vladas@system", cat: ""}
{"msgnr": 9, "lang": "EN", "msg": " Version 0.26.61. Linear Programming examples are finaly prepared and tested. The example algorithm uses quite rare combination of coordinate descent and conjugate gradient methods. I found it in <a href=\"https://books.google.lt/books/about/%D0%9F%D1%80%D0%B8%D0%B1%D0%BB%D0%B8%D0%B6%D0%B5%D0%BD%D0%BD%D0%BE%D0%B5_%D1%80%D0%B5%D1%88%D0%B5%D0%BD%D0%B8%D0%B5.html?id=nPpUAAAAYAAJ\"><i>Approximate solutions of control optimisation tasks</i></a> work of R.Fedorenko (\u00A748) in 1979, during my studies in the university. This algorithm was first implemented in April, 1979 in FORTRAN IV on IBM/360 mainframe. Later, in 1990, it was ported to C for IBM PC computers. And recently I've ported it to Javascript for this project.<br> <br> Here I'm representing 3 different examples of LP solutions for 100\u02DF500, 200\u02DF500 and 300\u02DF500 constraints matrices. Each example was calculated using 5 OPUs (each OPU is local CPU), which I've considered optimal for this kind of tasks. All parallel examples were processed 1.3 - 1.5 times faster than their sequential counterparts. Note here, that this improvement is not because of parallel behaviour of the task (so it doesn't depend much on OPUs count), but because of the increased convergention speed, caused by a number of aternative setups for each iteration, calculated in parallel. Although the speedup is not very significant in our cases, there chances exist that some matrices would show better and faster convergence, thus more speedup.<br><br>With these examples I've introduced some new <code>#pragma</code>s. The main constraints matrix in this algoritm does not mutate between iterations, so it would be logical to cache it in OPUs, instead of transferring it all the time through the network. I introduced <code>#pragma cache [var]</code> to take into account such cases. I've created a global cache in OPU's global space, for we could read cached variables directly into the executed code. The another <code>#pragma noautoparvar</code> has been introduced to disable default behavior of direct reading/writing of parallel variables from/to the server task space with locking. Sometimes (and it was a case in my example) there are only reads or writes w/wo locking needed for the particular tasks. Now it's possible to do it manually using this #pragma and with the help of  special internal functions.<br><br>Feel free to play with these examples, changing parameters, iterations context and initial matrices. Since <i>anonymous</i> account has a read-only filesystem, it's better to register your own account. It'll cost nothing for you, instead you'll have your own disk space for your experiments. The parameters for optimization task reside in the <code>linit.prm.json</code> file, and the matrix itself in the <code>linit.data.json</code> file. The latter file is generated when launching <code>linit.c (linit)</code> program, which you can compile on your own machine and, after execution, it'll create <code>linit.data.json</code> file, which content you then may copy/paste to the same file in our system. Happy hacking!", "subj": "Linear programming examples are available", "to_user": "@all", type: "P", sts: "", cdate: "19/05/19", ctime: 1000, fr_user: "vladas@system", cat: ""}
{"msgnr":10, "lang": "EN", "msg": " Version 0.30.70. This example is created to show the full power of Memel OS. Usually, ray tracing and rendering tasks are highly parallelizable by their nature, so it's logical to have them here as a good example for showing all advantages of the OPU parallel model.<br><br> This example was taken from <a href=\"https://avikdas.com/build-your-own-raytracer/\"><i>Buld your own 3D renderer</i></a>. It was then re-translated to ES5 standard. by removing such ES6+ syntax sugar like classes, arrow functions, <code>let</code>s and <code>const</code>s. These changes were needed because Memel OS currently supports only pre-ES6 standard, also known as Vanilla JS. Alsmost all features of ES6+ standards are a syntax sugar over existing ES5 constructs, so it's better to use Vanilla JS for better performance and clarity of concepts.<br><br>There are 2 examples created - one for small scene with 100 objects and one for big scene with 3000 objects. Both examples are presented in two variants - sequentail and parallel - for time comparations. The parallel solution was from 5 to 6 times faster than sequential one on 8 OPUs, depending on the scene size. The parallel algorithm scales very well and almost linearly with the number of processors (OPUs) increasing (tested with 2 to 8 OPUs). I'll test this algorithm more thoroughly in near future. I also plan to visualize the process of rendering with remote DOM subsystem, which is under development now. ", "subj": "Parallel ray tracing examples are available", "to_user": "@all", type: "P", sts: "", cdate: "19/09/14", ctime: 1000, fr_user: "vladas@system", cat: ""}
{"msgnr":11, "lang": "EN", "msg": " Version 0.35.78. __remoteDOM and future __remoteEVENT will be key and unique features of Memel OS. These are mechanisms of two-way communication between OPUs and the browser through the main system server. __remoteDOM is implemented as a set of Javascript operations with the browser DOM (as well as any other Javascript code) sent from OPUs to the server which then are retransmitted to the browser via websocket in real time. When the first __remoteDOM command from a particular task is encountered by browser, a separate tab is created for this task and all __remoteDOM commands are executed in an isolated task's context for this tab. HTML elements 'id's must be created and accessed by prepending a special task's internal variable '<code>__tid</code>' to an actual 'id' value. Thus, when several task's instances a launched all 'id's of HTML elements would be different for each instance.<br><br>Currently, a very simple and highly experimental model of __remoteDOM is implemented. You can see it in action when launching the examples of ray-tracing (in the <code>'examples/rendering'</code> directory). The resulting rendered image, built in the &lt;canvas&gt; of the browser, will be shown in the task's tab.<br><br>Have fun!", "subj": "Remote DOM for ray tracing", "to_user": "@all", type: "P", sts: "", cdate: "19/12/31", ctime: 1000, fr_user: "vladas@system", cat: ""}
