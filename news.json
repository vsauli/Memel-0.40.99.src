{"msgnr": 1, "lang": "EN", "msg": " The development of Project Memel V4.0 has started. There were 3 versions/attempts some time before, which were unsuccessful because I had no time to dedicate for the development. I tried various technologies for JS server and underlying database and, finally, I came up with using NodeJS and MongoDB for the current project version. The original idea for this project is based on my <a href='http://www.prodata.lt/EN/Programming/OPU_computing_model.pdf'>paper</a> about the possibility to parallelize Javascript code using a bunch of interconnected servers and workers (Object Processing Units - OPUs).<br> <br>The frontend for the Project is implemented as a Web SPA (Single Page Application) and uses ExtJS V3 widget library for GUI interface. I adopted an ACE web-based text editor for viewing and editing parallel JS executables and other files.<br> <br>The backend - the Parallel Javascript Machine - is using NodeJS, and implemented as a Web server for the frontend, as well as a server for the OPUs that really do the parallel execution of code. OPUs are small Javascript network clients implemented in NodeJS too. There may be pretty much of them, connected to the main parallel machine server either locally or remotely. The overall performance of parallel processing strongly depends on the number of connected OPUs.<br> <br> All these parts, working together, may be treated as a mini-OS which launches and parses the running scripts (tasks), puts them into system execution queue and provides some kind of cooperative multitasking. The results are printed to the Web client's console by pipelining <pre>console.log</pre> output from OPUs through the main server. <br> <br> Currently, at the version 4.0.0, the works are concentrated on a Web client software, which is responsible for creating, rditing and launching parallel Javascript tasks. <br> <br> --- Vladas Saulis<br> --- System architect and creator.<br>", "subj": "Project Memel 4.0 development has started!", "to_user": "@all", type: "P", sts: "", cdate: "17/01/30", ctime: 78565, fr_user: "vladas@system", cat: "M"}
{"msgnr": 2, "lang": "EN", "msg": " The Web GUI client is almost finished. Now system kernel development has started. Like any other OS, our mini-OS must have a kernel. The kernel operates with such internal structures as the Queue object, Task list and Task variables object.<br> <br> When the task is launched, its source code passed to <pre>Parallelizator</pre> function, where it separates into single statements (also can be grouped in blocks). Every statement or block then enqueued into the system's Queue for execution by OPUs, which are connected to the server and extract these statements from the Queue one by one in no predetermined order. So we may say that the tasks are parallelized on the <i>statement level</i>. The parallelization process is controlled by special instructions (or hints), called <pre>#pragmas</pre>. Some of them sets the start of parallel/sequential code, other sets beginnig/end of blocks, parallel variables (so called <i>Parvars</i>), begin/end of the process timing. The one of them, <pre>#pragma wait</pre>, is very important for the parallelization flow control, and marks the place of the <i>assembly point</i> in the task. At this point the parallelization of the task is suspended until all pending parallel chunks of code finishes their execution (for parallel loops it waits for all iterations are complete). After that the process of parallelization resumes from this point. The 'wait' process in our system is really non-blocking, letting other parts of the task to continue their execution, what is exactly opposite to normal one-threaded Javascript process. <br> <br> During the process of parallelization, when some statements contain an internal block of statements (such as loops, functions, if-then-else blocks), the <pre>Analizator</pre> function is called. It provides more specialized interpretaion of code logic. For example, it interprets '<pre>for</pre>' loop's initial expression, condition and step. This information then is added to the parallel code chunks as an extra internal code. <br> <br> Another important part of parallel tasks execution is the task variables handling. All task's variables are storing in the server Task object. Before the task chunk is placed into the Queue, the <pre>parallelizator</pre> function determines which variables are in use within this chunk. It puts these variables along with their values as a definition to the preamble of the code. After execution of the code chunk, the resulting values of variables are returned and set back to the server's task object. There are some special <i>parallel variables</i> (<pre>Parvars</pre>), which are handled differently. These variables are <i>declared</i> by <pre>#pragma parvar [var name]</pre> special instruction. This means, that these variables can be accessed and set concurrently. All 4 basic arithmetic operations are transitive (if processed one by one), so they can be processed in any order, and the result will always be the same. The parallel 'for' loop works exactly in this way. So the accumulator variable for the loop must be set in the way that it could be accessed and modified concurrently. This is what <pre>Parvars</pre> are for. The kernel adds additional code fragments that sinchronously calls server for variable value and locks it in order to be modified after the code chunk had finished its execution. It is important that the code chunk would be fast and atomic, so it would lock the parvar for as short time as possible. <br> <br> This is in a few words what I plan to implement in the next months. Hope it won't take a long time.", "subj": "Kernel development", "to_user": "@all", type: "P", sts:  "", cdate: "17/04/24", ctime: 79332, fr_user: "vladas@system", cat: "M"}
{"msgnr": 3, "lang": "EN", "msg": " OPU (Object Processing Unit) is a lightweight networking client (worker) which does the actual processing of parallelized chunks of code. OPU connects to main server and scans the Queue for the next job in FIFO manner. When it founds the job, it executes it, returns the result (task variables) and scans again. If no jobs are found it enters an idle loop and waits. Therefore OPUs are pro-active, because they initiate job extracting and processing. There may be an unlimited number of OPUs, connected to the main server, working independently. Their number is only limited by server's processing power and the network speed.<br> <br> OPUs have another very important function. When they meet Parvars they connect directly to the main server to get realtime value of Parvar, and after Parvar is changed they connect again and set its changed value to the corresponding task variable. While OPUs are processing Parvars in such way, these are locked on the server to prevent race condition. Thus, it could said, that the system has data bound control flow, which is opposite to the statement level control flow as it's usual for all non-parallel systems.<br> <br> When the OPU development will be finished, it will become possible to make the first <i>Proof of Concept</i> tests.", "subj": "OPU implementation", "to_user": "@all", type: "P", sts: "", cdate: "17/06/10", ctime: 1000, fr_user: "vladas@system", cat: ""}
{"msgnr": 4, "lang": "EN", "msg": " Multi-user access is very important for this Project. It lets users to launch tasks independently and asynchronously. Every task in the system belongs to a particular user, and only that user can see his task's output. When some user is registered in the system, he gets his own <i>workspace</i> with examples. He can then modify and save any file in his workspace, as well as to create new. The user's launched tasks may, in fact, execute in parallel. It depends on how often tasks waits for parallelization in <i>assembly points</i> (look at the previous news for this term). Such task behaviour is often called <i>cooperative multitasking</i>. First versions of Apple OS worked in similar way. All tasks in the system must give a 'breath' for all other tasks in the system, and the <pre>#pragma wait</pre> instruction is a way to do so.", "subj": "Multiuser access", "to_user": "@all", type: "P", sts: "", cdate: "17/06/10", ctime: 1000, fr_user: "vladas@system", cat: ""}
{"msgnr": 5, "lang": "EN", "msg": " Good news! The first <i>Proof of Concept</i> tests have passed successfully! The example of parallel <pre>for</pre> loop was tested with 1 and 2 OPUs on 2 processors Linux machine. It took 23.6 seconds with 1 OPU, and 13.5 seconds with 2 OPUs, each on separate CPU. It shows nearly linear performace gain for parallel <pre>for</pre> loop. Now it's planned to expose the whole Project Memel to the 'wild' for the public tests. 8 OUPs will be set up on 8 CPU machine, and the main server on the other. I plan to finish all works to the end of August, 2017. And as from now I have changed the version numbering. New versions will show the percentage of works done. By now the vesion is set to 0.10.14 alpha. The last number shows the build number. The Project is still in early alpha stage and pretends only for a 'Technology Preview'. If the community will find it useful in real parallel 'number-crunching' applications, the development will be continued.", "subj": "The first tests", "to_user": "@all", type: "P", sts: "", cdate: "17/06/12", ctime: 1000, fr_user: "vladas@system", cat: ""}
